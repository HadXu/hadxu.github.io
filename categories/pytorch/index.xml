<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>pytorch on lay&#39;s 博客</title>
    <link>https://hadxu.github.io/categories/pytorch/</link>
    <description>Recent content in pytorch on lay&#39;s 博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sat, 25 Apr 2020 09:00:00 +0800</lastBuildDate><atom:link href="https://hadxu.github.io/categories/pytorch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>再次思考深度学习框架的自动求导</title>
      <link>https://hadxu.github.io/post/2020-04-25-re-thinking-deep-learning-framework/</link>
      <pubDate>Sat, 25 Apr 2020 09:00:00 +0800</pubDate>
      
      <guid>https://hadxu.github.io/post/2020-04-25-re-thinking-deep-learning-framework/</guid>
      <description>再次思考深度学习框架的自动求导 刚开始读研的时候，对Pytorch以及Tensorflow的反向求导机制非常感兴趣，于是寻找了大量的资料去了解反向求导的机制。2017年华盛顿大学有一门课程非常棒dlsys,第一个实验关于自动求导，第二个实验关于将自动求导的矩阵计算使用GPU来操作。</description>
    </item>
    
    <item>
      <title>Pytorch Distributed Training</title>
      <link>https://hadxu.github.io/post/2020-04-10-Pytorch-distributed-training/</link>
      <pubDate>Fri, 10 Apr 2020 11:00:00 +0800</pubDate>
      
      <guid>https://hadxu.github.io/post/2020-04-10-Pytorch-distributed-training/</guid>
      <description>Pytorch Distributed Training Distributed data parallel training in Pytorch
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120  import os from datetime import datetime import argparse import torch.multiprocessing as mp import torchvision import torchvision.</description>
    </item>
    
  </channel>
</rss>
